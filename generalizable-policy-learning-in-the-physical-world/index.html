<!DOCTYPE html>
<html lang="en">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta charset="utf-8">
        <title>GPLPW2022</title>
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
        <div class="nav">
            <div class="nav-container">
                <a href="index.html#">Home</a>
                <a href="index.html#intro">Introduction</a>
                <a href="index.html#call">Call for Papers</a>
                <a href="index.html#challenge">Challenge</a>
                <a href="index.html#schedule">Schedule</a>
                <a href="index.html#speakers">Speakers</a>
                <a href="index.html#contact">Contact</a>
            </div>
        </div>

        <div class="title-container">
            <div style="text-align: center;">
                <div class="subtitle">The first workshop on</div>
                <h1>Generalizable Policy Learning in the Physical World</h1>
                <div class="subtitle" style="color: #ccc; margin: 20px">
                  April 29, ICLR 2022 Workshop (Virtual)
                </div>
            </div>
        </div>

        <div class="container">
            <div class="section" id="intro">
                <h2>Introduction</h2>

                <p>
                  Generalization is particularly important when <b>learning policies to interact with the physical world</b>. The spectrum of such policies is broad: the policies can be high-level, such as action plans that concern temporal dependencies and casualties of environment states; or low-level, such as object manipulation skills to transform objects that are rigid, articulated, soft, or even fluid.
                  In the physical world, an embodied agent can face a number of changing factors such as <b>physical parameters, action spaces, tasks, visual appearances of the scenes, geometry and topology of the objects</b>, etc. And many important real-world tasks involving generalizable policy learning, e.g., visual navigation, object manipulation, and autonomous driving. Therefore, learning generalizable policies is crucial to developing intelligent embodied agents in the real world. 
                </p>

                <!-- <p>
                  Though important, the field is very much under-explored in a systematic way. While the study of generalization has played an essential role in many application domains of machine learning (e.g., image recognition and natural language processing), it did not receive the same amount of attention in common frameworks of policy learning (e.g., reinforcement learning and imitation learning) at the early stage for reasons such as policy optimization is difficult and benchmark datasets are not quite ready yet. 
                </p> -->

                <!-- <p>
                  Learning generalizable policies in the physical world requires deep synergistic efforts across fields of <b>vision, learning, and robotics</b>, and poses many interesting research problems. <b>Recently, this field has started to attract much attention across disciplines</b>, with the maturity of policy optimization algorithms and emergence of training environments with diverse assets. 
                  For example, to achieve task-level generalization, meta-RL optimizes fast adaptation in unseen environments; to generalize past experience to new scenarios, offline RL learns an initial policy using previously collected trajectories, and the initial policy may be refined by an online RL process; to improve generalization over visual appearance, domain randomization and data augmentation techniques that were originally developed in computer vision are also applied in visual reinforcement/imitation learning. 
                  In addition, in this past year, we are witnessing new physical simulation environments with diverse assets for training and testing generalizabile policy learning algorithms.
                  Given the importance of this topic and the increasing attention attracted by it, we firmly believe that it is good timing to start the discussion of generalizable policy learning in ICLR, which is a pioneering conference which leads the trends of machine learning, especially applied machine learning research.
                </p> -->

                <p>
                  Learning generalizable policies in the physical world requires deep synergistic efforts across fields of <b>vision, learning, and robotics</b>, and poses many interesting research problems.
                  This workshop is designed to foster progress in generalizable policy learning, in particular, with <b>a focus on the tasks in the physical world</b>, such as visual navigation, object manipulation, and autonomous driving, because these real-world tasks require complex reasoning involving visual appearance, geometry, and physics. Technically, we expect to stimulate improvement in new directions such as:
                  <ul>
                    <li>Emergence of physical concepts driven by interaction</li>
                    <li>Physical concept-based policy making</li>
                    <li>3D learning for policy learning </li>
                    <li>Differentiable physics coupled with policy learning</li>
                    <li>...</li>
                  </ul>
                </p>


                <p>
                  Our main targeted participants are researchers interested in <b>applying learning methods to develop intelligent embodied agents in the physical world</b>. More specifically, target communities include, but are not limited to: 
                  <i>
                    robotics, reinforcement learning, learning from demonstrations, offline reinforcement learning, meta-learning, multi-task learning, 3D vision, computer vision, computer graphics, and physical simulation.
                  </i>
                </p>

                <p>
                  In affiliation to this workshop, we are also organizing the <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">ManiSkill Challenge</a>, which focuses on learning to manipulate unseen objects in simulation with 3D visual inputs. We will announce winners and host winner presentations in this workshop.
                </p>

            </div>

            <div class="section" id="call">
              <h2>Call for Papers</h2>
              <p>
                We invite submission to the Generalizable Policy Learning in the Physical World workshop, hosted at ICLR 2022. 
              </p>
              <h3>Paper topics</h3>
              <p>
              A non-exhaustive list of relevant topics:
              <ul>
                <li>Learning methods for embodied AI tasks (e.g., manipulation, navigation)</li>
                <li>Real-world or simulated benchmarks for generalizable policy learning</li>
                <li>Learning representations for generalization in physical world tasks</li>
                <li>Large-scale reinforcement/imitation learning</li>
                <li>Multi-task learning</li>
                <li>Data augmentation techniques for generalizable policy learning</li>
                <li>Few-shot imitation learning</li>
                <li>Affordance prediction</li>
                <li>Other topics about generalizable policy learning in the physical world</li>
              </ul>
              </p>
              <h3>Submission Guidelines</h3>
              <p>
                <ul>
                  <li><b>Submission Portal:</b> <a href="https://openreview.net/group?id=ICLR.cc/2022/Workshop/GPL">OpenReview</a></li>
                  <!-- <li><b>Format:</b> You must format your submission using the <a href="https://github.com/ICLR/Master-Template/raw/master/archive/iclr2022.zip"> ICLR 2022 LaTeX style file.</a> </li> -->
                  <li><b>Paper Length:</b> 
                    Submissions could be either <b>4-page</b> short papers or <b>8-page</b> long papers, excluding references, acknowledgements, and appendices. 
                  </li>
                  <li><b>Format:</b>
                    <ul>
                      <li>You must format your submission using the <a href="../assets/gpl_iclr2022.zip"> updated ICLR 2022 LaTeX style file.</a></li>
                      <li>Please include the references and supplementary materials in the same PDF as the main paper. </li>
                      <li>The maximum file size for submissions is 100MB. Submissions that violate the ICLR style (e.g., by decreasing margins or font sizes) or page limits may be rejected without further review. </li>
                    </ul>
                  </li>
                  <li><b>Dual Submission:</b>
                    <ul>
                      <li>Papers to be submitted or in preparation for submission to other major venues (including ICML 2022) in the field are allowed.</li>
                      <li>We also weclome published works, but they must be explicitly stated at the time of submission.</li>
                    </ul>
                  </li>
                  <li><b>Non-archival:</b> The workshop is a non-archival venue and will not have official proceedings. Workshop submissions can be subsequently or concurrently submitted to other venues.</li>
                  <li><b>Visibility:</b> Submissions and reviews will not be public. Only accepted papers will be made public.</li>
                  <li>We encourage the participants of the affiliated challenge to submit technical reports which summarizing their solutions.</li>
                  <li><b>Contact:</b> <a href="mailto:iclr2022gpl@gmail.com">iclr2022gpl@gmail.com</a></li>
                </ul>
              </p>

              <h3>Review and Selection</h3>
              <p>
                <ul>
                  <li>The review process will be double-blind. As an author, you are responsible for anonymizing your submission. In particular, you should not include author names, author affiliations, or acknowledgements in your submission and you should avoid providing any other identifying information (even in the supplementary material). </li>
                  <li>Each submission will be reviewed for originality, significance, clarity, soundness, relevance and technical contents.</li>
                  <li>Accepted submissions will be presented in the form of posters or contributed talks. </li>
                  <li>Authors of accepted papers will be required to record a 5-minute video. </li>
                  <li>At least one co-author of each accepted paper is expected to register for ICLR 2022 and attend the poster session. </li>
                  <li>All the accepted submissions will be available on our workshop website, though authors could indicate explicitly if they want to opt out. </li>
                </ul>
              </p>
       
              <h3>Timeline (11:59 PM Pacific Standard Time)</h3>
              <table style="width: 100%">
                  <tr><td>Jan 17, 2022</td><td>Announcement and call for submissions</td></tr>
                  <tr><td><s>Feb 25</s><b><font color=red> Mar 2, 2022</font></b></td><td>Paper submission <b>deadline</b></td></tr>
                  <tr><td>Mar 25, 2022</td><td>Review decisions announced</td></tr>
                  <tr><td>Apr 8, 2022</td><td>Camera ready deadline</td></tr>
              </table>
          </div>


            <div class="section" id="challenge">
                <h2>Challenge</h2>
                Please refer to the <a href="https://sapien.ucsd.edu/challenges/maniskill2021/">ManiSkill Challenge website</a> for details.
            </div>

            <div class="section" id="schedule">
                <h2>Workshop Schedule</h2>
                (coming soon)
                <!-- <table style="width: 100%">
                     <tr> <th>Start</th> <th>End</th> <th>Event</th> </tr>
                     <tr><td>8:50am</td><td>9:00am</td><td>Opening Remark</td></tr>
                     <tr><td>9:00am</td><td>9:30am</td><td>Keynote speaker</td></tr>
                     <tr><td>9:30am</td><td>10:00am</td><td>Keynote speaker</td></tr>
                     <tr><td>10:00am</td><td>10:40am</td>
                     <td>Spottrght Section: Simulation Environments for Embodied AI</td></tr>
                     <tr><td>10:40am</td><td>11:10am</td><td>Keynote speaker</td></tr>
                     <tr><td>11:10am</td><td>11:40am</td><td>Keynote speaker</td></tr>
                     <tr><td>11:40am</td><td>12:10pm</td><td>Keynote speaker</td></tr>
                     <tr><td>12:10pm</td><td>1:30pm</td><td>Lunch</td></tr>
                     <tr><td>1:30pm</td><td>2:00pm</td><td>Keynote speaker</td></tr>
                     <tr><td>2:00pm</td><td>2:30pm</td><td>Keynote speaker</td></tr>
                     <tr><td>2:30pm</td><td>3:00pm</td><td>Keynote speaker</td></tr>
                     <tr><td>3:00pm</td><td>3:40pm</td><td>Accepted Paper Spottrght Presentation</td></tr>
                     <tr><td>3:40pm</td><td>4:10pm</td><td>Keynote speaker</td></tr>
                     <tr><td>4:10pm</td><td>4:40pm</td><td>Keynote speaker</td></tr>
                     <tr><td>4:40pm</td><td>5:10pm</td><td>Panel Discussion and Community Building</td></tr>
                     </table> -->
                <ul>
                </ul>
            </div>

            <div class="section" id="speakers">
                <h2>Invited Speakers</h2>
                <p> listed alphabetically </p>
                <div class="people">
                  <a href="https://nbfigueroa.github.io/">
                    <img src="../assets/nadia_figueroa.jpg">
                    <div>Nadia Figueroa</div>
                    <div class="aff">UPenn</div>
                  </a>
                  <a href="https://research.google/people/MrinalKalakrishnan/">
                    <img src="../assets/mrinal_kalakrishnan.png">
                    <div>Mrinal Kalakrishnan</div>
                    <div class="aff">X</div>
                  </a>
                  <a href="https://www.csc.kth.se/~danik/">
                    <img src="../assets/danica_kragic.png">
                    <div>Danica Kragic</div>
                    <div class="aff">KTH</div>
                  </a>
                  <a href="https://www.cs.columbia.edu/~shurans/">
                    <img src="../assets/shuran_song.jpg">
                    <div>Shuran Song</div>
                    <div class="aff">Columbia</div>
                  </a>
                  <a href="https://www.cs.utexas.edu/~pstone/">
                    <img src="../assets/peter_stone.png">
                    <div>Peter Stone</div>
                    <div class="aff">UT Austin and Sony AI</div>
                  </a>
                  <a href="https://xiaolonw.github.io/">
                    <img src="../assets/xiaolong_wang.jpg">
                    <div>Xiaolong Wang</div>
                    <div class="aff">UC San Diego</div>
                  </a>
                </div>
                <div class="people">
                </div>
            </div>

            <div class="section" id="organizers">
              <h2>Organizers</h2>
              <p> listed alphabetically </p>
              <div class="people">
                <a href="https://3d.snu.ac.kr/members/">
                  <img src="../assets/youngmin_kim.jpeg">
                  <div>Young Min Kim</div>
                  <div class="aff">Seoul National University</div>
                </a>
                <a href="https://www.cs.umd.edu/~lin/">
                  <img src="../assets/Ming_Lin_UMIACS.jpg">
                  <div>Ming C. Lin</div>
                  <div class="aff">University of Maryland College Park</div>
                </a>
                <a href="https://people.eecs.berkeley.edu/~svlevine/">
                  <img src="../assets/sl.png">
                  <div>Sergey Levine</div>
                  <div class="aff">UC Berkeley</div>
                </a>
                <a href="http://cseweb.ucsd.edu/~t3mu/">
                  <img src="../assets/tongzhoumu.jpg">
                  <div>Tongzhou Mu</div>
                  <div class="aff">UC San Diego</div>
                </a>
                <a href="https://ashvin.me/">
                  <img src="../assets/ashvin.jpg">
                  <div>Ashvin Nair</div>
                  <div class="aff">UC Berkeley</div>
                </a>
                <a href="https://cseweb.ucsd.edu/~haosu/">
                  <img src="../assets/haosu.jpeg">
                  <div>Hao Su</div>
                  <div class="aff">UC San Diego</div>
                </a>
              </div>
            </div>

            <div class="section" id="reviewers">
              <h2>Program Committee</h2>
              <p> We would like to thank the following people for their effort in providing feedback for submissions! </p>
              <ul>
                <li>Abhishek Gupta</li>
                <li>Ankur Handa</li>
                <li>Annie Xie</li>
                <li>Anurag Ajay</li>
                <li>Avi Singh</li>
                <li>Brijen Thananjeyan</li>
                <li>Cheol-Hui Min</li>
                <li>Coline Devin</li>
                <li>Dhruv Shah</li>
                <li>Dongsu Zhang</li>
                <li>Fanbo Xiang</li>
                <li>Fangchen Liu</li>
                <li>Fei Liu</li>
                <li>Homer Walke</li>
                <li>Jiayuan Gu</li>
                <li>Jonathan Yang</li>
                <li>Junbang Liang</li>
                <li>Krishna Murthy</li>
                <li>Laura Smith</li>
                <li>Liyiming Ke</li>
                <li>Miles Macklin</li>
                <li>Minghua Liu</li>
                <li>Nicklas Hansen</li>
                <li>Quan Vuong </li>
                <li>Rui Chen</li>
                <li>Sasha Khazatsky</li>
                <li>Shikhar Bahl</li>
                <li>Shuang Liu</li>
                <li>Tao Chen</li>
                <li>Tianhe Yu</li>
                <li>Vikash Kumar</li>
                <li>Weizi Li</li>
                <li>Xuanlin Li</li>
                <li>Yoonseon Oh</li>
                <li>Yu Shen</li>
                <li>Yufei Ye</li>
                <li>Zhan Ling</li>
                <li>Zhiao Huang</li>
                <li>Zhiwei Jia</li>
                <li>Zih-Yun Chiu</li>
              </ul>
            </div>

            <div class="section" id="postersessions">
              <h2>Poster Sessions</h2>
              <p>Poster session assignments are posted below. Poster session A is 9:20-10:15 PDT. Poster session B is 15:50-16:45 PDT. The session will be at <a href="https://app.gather.town/app/Wfl5hBvVzs7ELFNS/gplpw-poster-room">https://app.gather.town/app/Wfl5hBvVzs7ELFNS/gplpw-poster-room</a>.
                
              <table style="width: 100%">
              <thead>
                <tr>
                  <th class="tg-7zrl">Session A</th>
                  <th class="tg-7zrl">9:20-10:15 PDT</th>
                  <th class="tg-0lax">Session B</th>
                  <th class="tg-0lax">15:50-16:45 PDT</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td class="tg-2b7s">1</td>
                  <td class="tg-0lax">A Minimalist Ensemble Method for Generalizable Offline Deep Reinforcement Learning</td>
                  <td class="tg-lqy6">2</td>
                  <td class="tg-0lax">Sim-to-Lab-to-Real: Safe RL with Shielding and Generalization Guarantees</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">4</td>
                  <td class="tg-0lax">Learning Generalizable Dexterous Manipulation from Human Grasp Affordance</td>
                  <td class="tg-lqy6">3</td>
                  <td class="tg-0lax">Continuous Control on Time</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">6</td>
                  <td class="tg-0lax">An Empirical Study and Analysis of Learning Generalizable Manipulation Skill in the SAPIEN Simulator</td>
                  <td class="tg-lqy6">5</td>
                  <td class="tg-0lax">Don't Change the Algorithm, Change the Data: Exploratory Data for Offline Reinforcement Learning</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">7</td>
                  <td class="tg-0lax">Revisiting Model-based Value Expansion</td>
                  <td class="tg-lqy6">8</td>
                  <td class="tg-0lax">Let’s Handle It: Generalizable Manipulation of Articulated Objects</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">11</td>
                  <td class="tg-0lax">Control of Two-way Coupled Fluid Systems with Differentiable Solvers</td>
                  <td class="tg-lqy6">9</td>
                  <td class="tg-0lax">Versatile Offline Imitation Learning via State-Occupancy Matching</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">12</td>
                  <td class="tg-0lax">PAnDR: Fast Adaptation to New Environments from Offline Experiences via Decoupling Policy and Environment Representations</td>
                  <td class="tg-lqy6">10</td>
                  <td class="tg-0lax">One-Shot Imitation with Skill Chaining using a Goal-Conditioned Policy in Long-Horizon Control</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">14</td>
                  <td class="tg-0lax">Know Thyself: Transferable Visual Control Policies Through Robot-Awareness</td>
                  <td class="tg-lqy6">13</td>
                  <td class="tg-0lax">Density Estimation For Conservative Q-Learning</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">16</td>
                  <td class="tg-0lax">Compositional Multi-Object Reinforcement Learning with Linear Relation Networks</td>
                  <td class="tg-lqy6">18</td>
                  <td class="tg-0lax">A Probabilistic Perspective on Reinforcement Learning via Supervised Learning</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">17</td>
                  <td class="tg-0lax">Prompts and Pre-Trained Language Models for Offline Reinforcement Learning</td>
                  <td class="tg-lqy6">22</td>
                  <td class="tg-0lax">Silver-Bullet-3D at ManiSkill 2021: Learning-from-Demonstrations and Heuristic Rule-based Methods for Object Manipulation</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">19</td>
                  <td class="tg-0lax">Invariant Causal Representation Learning for Generalization in Imitation and Reinforcement Learning</td>
                  <td class="tg-lqy6">23</td>
                  <td class="tg-0lax">Learning Robust Task Context with Hypothetical Analogy-Making</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">20</td>
                  <td class="tg-0lax">Reinforcement Learning for Location-Aware Warehouse Scheduling</td>
                  <td class="tg-lqy6">26</td>
                  <td class="tg-0lax">Improving performance on the ManiSkill Challenge via Super-convergence and Multi-Task Learning</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">21</td>
                  <td class="tg-0lax">Zero-Shot Reward Specification via Grounded Natural Language</td>
                  <td class="tg-lqy6">27</td>
                  <td class="tg-0lax">ShiftNorm: On Data Efficiency in Reinforcement Learning with Shift Normalization</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">24</td>
                  <td class="tg-0lax">Deep Sequenced Linear Dynamical Systems for Manipulation Policy Learning</td>
                  <td class="tg-lqy6">29</td>
                  <td class="tg-0lax">Separating the World and Ego Models for Self-Driving</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">25</td>
                  <td class="tg-0lax">Multi-task Reinforcement Learning with Task Representation Method</td>
                  <td class="tg-lqy6">33</td>
                  <td class="tg-0lax">Generalizable Reinforcement Learning for Automated Driving with Hidden Parameter Block MDPs</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">28</td>
                  <td class="tg-0lax">Multi-objective evolution for Generalizable Policy Gradient Algorithms</td>
                  <td class="tg-lqy6">34</td>
                  <td class="tg-0lax">Using Deep Learning to Bootstrap Abstractions for Robot Planning</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">30</td>
                  <td class="tg-0lax">Safer Autonomous Driving in a Stochastic, Partially-Observable Environment by Hierarchical Contingency Planning</td>
                  <td class="tg-lqy6">35</td>
                  <td class="tg-0lax">Generalization in Tracking and Adaptation for a Model-based-feedforward and Iterative-learning Controller</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">32</td>
                  <td class="tg-0lax">Don't Freeze Your Embedding: Lessons from Policy Finetuning in Environment Transfer</td>
                  <td class="tg-lqy6">37</td>
                  <td class="tg-0lax">Planning to Practice: Efficient Online Fine-Tuning by Composing Goals in Latent Space</td>
                </tr>
                <tr>
                  <td class="tg-2b7s">36</td>
                  <td class="tg-0lax">Learning Transferable Policies By Inferring Agent Morphology</td>
                  <td class="tg-lqy6">38</td>
                  <td class="tg-0lax">A Study of Off-Policy Learning in Environments with Procedural Content Generation</td>
                </tr>
                <tr>
                  <td class="tg-7zrl"></td>
                  <td class="tg-7zrl"></td>
                  <td class="tg-lqy6">39</td>
                  <td class="tg-0lax">Learning Category-Level Generalizable Object Manipulation Policy via Generative Adversarial Self-Imitation Learning from Demonstrations</td>
                </tr>
                <tr>
                  <td class="tg-0lax"></td>
                  <td class="tg-7zrl"></td>
                  <td class="tg-lqy6">40</td>
                  <td class="tg-0lax">FlexiBiT: Flexible Inference in Sequential Decision Problems via Bidirectional Transformers</td>
                </tr>
                <tr>
                  <td class="tg-2b7s"></td>
                  <td class="tg-0lax"></td>
                  <td class="tg-lqy6">41</td>
                  <td class="tg-0lax">Imitation Learning for Generalizable Self-driving Policy with Sim-to-real Transfer</td>
                </tr>
              </tbody>
              </table>
            </div>

            <div class="section" id="contact">
                <h2>Contact</h2>
                <div>For any questions, you may contact us at <a href="mailto:iclr2022gpl@gmail.com">iclr2022gpl@gmail.com</a></div>
            </div>

        </div>
        <div class="foot">
          © 2022 Generalizable Policy Learning in the Physical World
        </div>
    </body>
</html>
