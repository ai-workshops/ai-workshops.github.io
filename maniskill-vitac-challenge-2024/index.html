<!DOCTYPE html>
<html lang="en">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta charset="utf-8">
        <title>ManiSkill-ViTac Challenge 2024</title>
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
        <div class="nav">
            <div class="nav-container">
                <!-- <a href="index.html#">Home</a> -->
                <a href="index.html#intro">Introduction</a>
                <a href="index.html#start">Getting Started</a>
                <a href="index.html#contact">Contact</a>
                <a href="index.html#tasks">Tasks</a>
                <a href="index.html#format">Format</a>
                <a href="index.html#metrics">Metrics</a>
                <a href="index.html#awards">Winners</a>
                <a href="index.html#schedule">Schedule</a>
                <a href="index.html#organizers">Organizers</a>
            </div>
        </div>

        <div class="title-container">
            <div style="text-align: center; margin: 20px">
                <h1>ManiSkill-ViTac: Vision-based-Tactile Manipulation Skill Learning Challenge 2024</h1>
            </div>
        </div>

        <div class="container">
            <div class="section" id="intro">
                <h2>Introduction</h2>

                <p>
                  <b>Vision-based tactile sensing</b> has witnessed great progress in recent years. It can provide contact information which is important for manipulation skill learning. However, many research labs, particularly those lacking hardware expertise, face challenges in building physical experimental platforms and concentrating their research efforts on manipulation skill learning.
                </p>
                <p>
                  The <b>ManiSkill-ViTac Challenge</b> aims to provide a standardized benchmarking platform for evaluating the performance of vision-based-tactile manipulation skill learning in real-world robot applications. This challenge focuses specifically on assessing skill learning. Therefore, to enable fair comparison between approaches, the challenge uses a unified tactile sensor type and simulator across all participants. 
                </p>

                <p>
                  To the best of our knowledge, it is the first open challenge in this field. We hope it can bring together world-wide researchers from tactile sensing and policy learning and help nurture the development of advanced manipulation skill learning frameworks based on tactile sensing. 
                </p>

                <p>
                  <b>
                  The award ceremony will be held at <a href="https://shanluo.github.io/ViTacWorkshops/"> the 5th ViTac workshop</a> in <a href="https://2024.ieee-icra.org/">ICRA 2024</a>.
                  </b>
                  
                </p>

                <p>
                  The ManiSkill-ViTac Challenge features:
                  <ul>
                    <li>An open-source general-purpose tactile sensor physics simulator</li>
                    <li>Real-world robot evaluation</li>
                    <li>Two high-precision manipulation tasks that closely resemble real-world applications</li>
                  </ul>
                </p>

            </div>
            <div class="section" id="start">
              <h2>Getting Started</h2>
              <p>To get started submitting to the challenge, follow these steps:</p>

            <p>1. Register a team to make submissions. Send an email to <a href="mailto:maniskill.vitac@gmail.com?subject=ManiSkill-ViTac Team Registration [Team Name]">maniskill.vitac@gmail.com</a>. We recommend using education emails for registration. Teams shall have no more than one leader and one instructor. The maximum number of additional team members is four.
            </p>
            <p>
              Email format:
              <pre>
                Team Name: [Team Name]
                Team Leader: [Name] [Institute] [Email]
                (Optional) Instructor: [Name] [Institute] [Email]
                Team Members:
                1. [Name] [Institue] [Email (optional)]
                2. [Name] [Institue] [Email (optional)]
                3. [Name] [Institue] [Email (optional)]
                4. [Name] [Institue] [Email (optional)]
              </pre>
            </p>
            <p>
              2. The codebase for this challenge is hosted on <a href="https://github.com/callmeray/ManiSkill-ViTac2024">GitHub</a>. This repository contains information about the competition environments, and the submission format.
            </p>
            <p>
              3. Checkout the <a href="https://docs.google.com/spreadsheets/d/1ZCNSbctm5eyr4Q59KmVBE0ZMo5mt63emFLihbJn1maw/edit?usp=sharing">leaderboard</a> and make your own submission.
            </p>
            </div>
            <div class="section" id="contact">
              <h2>Contact</h2>
              Join our <a href="https://discord.gg/nvpZqB6J">discord</a> to contact us. You may also email us at  <a href="mailto:maniskill.vitac@gmail.com?subject=ManiSkill-ViTac Team Registration [Team Name]">maniskill.vitac@gmail.com</a>.
            </div>
            <div class="section" id="tasks">
              <h2>Tasks</h2>
              <p>
                The challenge includes two tasks: peg insertion and lock opening. Similar to the  <a href="https://sapien.ucsd.edu/challenges/maniskill/">ManiSkill challenge</a>, we focus on evaluating the policy’s generalizability. Each task includes multiple objects of different shapes (e.g., the peg can be a cuboid or a trapezoid, and keys can have two or three teeth), which are unknown during evaluation. Therefore, the policy needs to be able to generalize to different plugs and keys.
              </p>
              <div style="text-align: center;">
                <img src="peg-insertion-sim2real.gif" alt="Plug insertion"  width="600">
                <figcaption>Peg insertion</figcaption>
                <img src="lock-opening-sim2real.gif" alt="Lock opening"  width="600">
                <figcaption>Lock opening</figcaption>
              </div>
            </div>
            <div class="section" id="format">
              <h2>Challenge Format</h2>
              <p>
                The challenge is in two phases: Phase 1 Simulation, Phase 2 Real-world
              </p>
              <h3>Phase 1</h3>
              <p>
              The simulation environments are built based on <a href="https://ieeexplore.ieee.org/abstract/document/10388459/">our developed tactile sensor simulator</a>, which uses FEM (Finite Element Method) for physics simulation and <a href="https://ipc-sim.github.io/">IPC (Incremental Potential Contact)</a> as the contact model. 
              <h3>Phase 2</h3>
              <p>
              The top <i>N</i> participants’ algorithms will be evaluated on a physical robot. We use <a href="https://www.gelsight.com/gelsightmini/">GelSight Mini</a>, a commercially available vision-based tactile sensor, in our challenge. The input and output format of the real environment will be the same as the simulation. 
              <div style="text-align: center;">
                <img src="pen_in_hole.png" alt=""  height="400">
                </div>
              </p>
              <p>
              Each team will have 3 chances to upload the codes and evaluate. Full evaluation logs, including raw observations, groundtruth states, will be given to the participants, such that they can fine-tune their algorithms.
              </p>
            </div>

            <div class="section" id="metrics">
              <h2>Metrics</h2>
              <p>
                Participating entries will be evaluated on <i>K</i> initial states. Each state will repeat <i>T</i> times. An episode is successful if the peg is in the hole and all the pins in the lock are lifted up. For the real-world robot, a torque sensor is used. If at any point the torque on the gripper is larger than a threshold, the task fails immediately. Then, we will have an array of success rates. We use the average rank of the <i>K</i> initial states as the final metric for comparing submissions.
              </p>
              <p>
                For Phase1, the participants run the evaluation script on their own computer and submit the log files to the challenge committee.
              </p>
              <p>
                We reserve the right to use additional metrics to choose winners in case of statistically insignificant differences.
              </p>
            </div>
            <div class="section" id="awards">
              <h2>Winners</h2>
                1st Prize: 
                  <li>
                    Team Illusion, Institute: GuangXi University
                  </li>
                2nd Prize:
                <li>Team Luban, Institute: Zhejiang University</li>
                <li>Team TouchSight Innovators, Institute: Tongji University, King's College London</li>

                3rd Prize:
                <li>Team SHT, Institute: ShanghaiTech University</li>
                <li>Team Power *Star, Institute: Nanyang Technological University, I2R, ASTAR</li>
                <li>Team GXU-ICMPE,  Institute: GuangXi University</li>
                <li>Team SSR-Tac, Institute: Tsinghua-Berkeley Shenzhen Institute, Tsinghua University</li>

              </div>
            <div class="section" id="schedule">
              <h2>Schedule</h2>
              <ul>
                <li>2024/01/31 Challenge release
                </li>
                <li>2024/03/01 Phase 1 Start</li>
                <li>2024/03/31 Phase 1 Stage 1 Deadline
                </li>
                <li>2024/04/01 Phase 2 Stage 2 Start</li>
                <li>2024/04/13 Phase 1 Stage 2 Deadline</li>
                <li>2024/04/30 Phase 2 Deadline
                </li>
                <li>2024/05/01 Award Announcement</li>
                <li>2024/05/13 Award Ceremony on <a href="https://shanluo.github.io/ViTacWorkshops/"> the 5th ViTac workshop</a> in <a href="https://2024.ieee-icra.org/">ICRA 2024</a>
                </li>
                <p>The winner of the 1st Prize will be invited to deliver a 15-minute presentation at the workshop, showcasing their achievements and insights.</p>
                <p>In Phase 1 Stage 1, the top 3 teams on the leaderboard will progress to Phase 2, each with 5 submission opportunities in Phase 2.</p>

                <p>In Phase 1 Stage 2, the top 3 teams on the leaderboard (excluding those that have already advanced from Stage 1) will progress to Phase 2, with each team receiving 3 submission opportunities in Phase 2.</p>

              </ul>
            </div>
            <div class="section" id="organizers">
                <h2>Organizers</h2>
                <h3>Faculty</h3>
                <div class="people">
                  <a href="https://callmeray.github.io/homepage/">
                    <img src="../assets/rui_chen.png">
                    <div>Rui Chen</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="https://cseweb.ucsd.edu/~haosu/">
                    <img src="../assets/haosu.jpeg">
                    <div>Hao Su</div>
                    <div class="aff">UC San Diego</div>
                  </a>                  
                  <a href="https://shanluo.github.io/">
                    <img src="../assets/shan_luo.png">
                    <div>Shan Luo</div>
                    <div class="aff">King's College London</div>
                  </a>
                  <a href="https://lepora.com/">
                    <img src="../assets/nathan_lepora.jpg">
                    <div>Nathan Lepora</div>
                    <div class="aff">University of Bristol</div>
                  </a>
                  <a href="https://lasr.org/">
                    <img src="../assets/roberto_calandra.jpg">
                    <div>Roberto Calandra</div>
                    <div class="aff">TU Dresden</div>
                  </a>
                  <a href="https://www.me.tsinghua.edu.cn/en/info/1081/1202.htm">
                    <img src="../assets/jing_xu.jpg">
                    <div>Jing Xu</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                </div>
                <h3>Student</h3>
                <div class="people">
                  <a href="">
                    <img src="../assets/weihang_chen.jpg">
                    <div>Weihang Chen</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="">
                    <img src="../assets/yuhao_liu.jpg">
                    <div>YuHao Liu</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="https://rabbit-hu.github.io/">
                    <img src="../assets/xiaodi_yuan.jpg">
                    <div>Xiaodi Yuan</div>
                    <div class="aff">UC San Diego</div>
                  </a>
                  <a href="">
                    <img src="../assets/chuanyu_li.jpg">
                    <div>Chuanyu Li</div>
                    <div class="aff">Sichuan University</div>
                  </a>                  
                  <a href="https://www.fbxiang.com">
                    <img src="../assets/fanbo_xiang.jpg">
                    <div>Fanbo Xiang</div>
                    <div class="aff">UC San Diego</div>
                  </a>
                  <a href="">
                    <img src="../assets/mai_zhang.jpg">
                    <div>Mai Zhang</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                </div>
            </div>
            <h2>Citation</h2>
            <p>If you use our work in your research please cite the following</p>
            <pre>@ARTICLE{chen2024tactilesim2real,
              author={Chen, Weihang and Xu, Jing and Xiang, Fanbo and Yuan, Xiaodi and Su, Hao and Chen, Rui},
              journal={IEEE Transactions on Robotics}, 
              title={General-Purpose Sim2Real Protocol for Learning Contact-Rich Manipulation With Marker-Based Visuotactile Sensors}, 
              year={2024},
              volume={},
              number={},
              pages={1-18},
              doi={10.1109/TRO.2024.3352969}}
            </pre>
        </div>
        
        <div class="foot">
          © ManiSkill-ViTac: Vision-based-Tactile Manipulation Skill Learning Challenge 2024
        </div>
    </body>
</html>
