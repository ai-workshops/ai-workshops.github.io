<!DOCTYPE html>
<html lang="en">
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <meta charset="utf-8">
        <title>ManiSkill-ViTac Challenge 2024</title>
        <link rel="stylesheet" href="style.css">
    </head>
    <body>
        <div class="nav">
            <div class="nav-container">
                <!-- <a href="index.html#">Home</a> -->
                <a href="index.html#intro">Introduction</a>
                <a href="index.html#start">Getting Started</a>
                <a href="index.html#format">Format</a>
                <a href="index.html#tasks">Tasks</a>
                <a href="index.html#awards">Awards</a>
                <a href="index.html#metrics">Metrics</a>
                <a href="index.html#schedule">Schedule</a>
                <a href="index.html#organizers">Organizers</a>
            </div>
        </div>

        <div class="title-container">
            <div style="text-align: center; margin: 20px">
                <h1>ManiSkill-ViTac: Vision-based-Tactile Manipulation Skill Learning Challenge 2024</h1>
            </div>
        </div>

        <div class="container">
            <div class="section" id="intro">
                <h2>Introduction</h2>

                <p>
                  <b>Vision-based tactile sensing</b> has witnessed great progress in recent years. It can provide contact information which is important for manipulation skill learning. However, many research labs, particularly those lacking hardware expertise, face challenges in building physical experimental platforms and concentrating their research efforts on manipulation skill learning.
                </p>
                <p>
                  The <b>ManiSkill-ViTac Challenge</b> aims to provide a standardized benchmarking platform for evaluating the performance of vision-based-tactile manipulation skill learning in real-world robot applications. This challenge focuses specifically on assessing skill learning. Therefore, to enable fair comparison between approaches, the challenge uses a unified tactile sensor type and simulator across all participants. 
                </p>

                <p>
                  To the best of our knowledge, it is the first open challenge in this field. We hope it can bring together world-wide researchers from tactile sensing and policy learning and help nurture the development of advanced manipulation skill learning frameworks based on tactile sensing. 
                </p>

                <p>
                  <b>
                  The award ceremony will be held at the 5th ViTac workshop in <a href="https://2024.ieee-icra.org/">ICRA 2024</a>.
                  </b>
                  
                </p>

                <p>
                  The ManiSkill-ViTac Challenge features:
                  <ul>
                    <li>An open-source general-purpose tactile sensor physics simulator</li>
                    <li>Real-world robot evaluation</li>
                    <li>Two high-precision manipulation tasks that closely resemble real-world applications</li>
                  </ul>
                </p>

            </div>
            <div class="section" id="start">
              <h2>Getting Started</h2>
              <p>To get started submitting to the challenge, follow these steps:</p>

            <p>1. Register a team to make submissions. Send an email to <a href="mailto:maniskill.vitac@gmail.com?subject=ManiSkill-ViTac Team Registration [Team Name]">maniskill.vitac@gmail.com</a>. We recommend using education emails for registration. Teams shall have no more than one leader and one instructor. The maximum number of additional team members is four.
            </p>
            <p>
              Email format:
              <pre>
                Team Name: [Team Name]
                Team Leader: [Name] [Institute] [Email]
                (Optional) Instructor: [Name] [Institute] [Email]
                Team Members:
                1. [Name] [Institue] [Email (optional)]
                2. [Name] [Institue] [Email (optional)]
                3. [Name] [Institue] [Email (optional)]
                4. [Name] [Institue] [Email (optional)]
              </pre>
            </p>
            <p>
              2. The codebase for this challenge is hosted on <a href="https://github.com/callmeray/ManiSkill-ViTac2024">GitHub</a>. This repository contains information about the competition environments, and the submission format.
            </p>
            <p>
              3. Checkout the <a href="https://docs.google.com/spreadsheets/d/1ZCNSbctm5eyr4Q59KmVBE0ZMo5mt63emFLihbJn1maw/edit?usp=sharing">leaderboard</a> and make your own submission.
            </p>
            </div>
            <div class="section" id="format">
              <h2>Challenge Format</h2>
              <p>
                The challenge is in two phases: Phase 1 Simulation, Phase 2 Real-world
              </p>
              <h3>Phase 1</h3>
              <p>
              The simulation environments are built based on our developed tactile sensor simulator. The simulator uses FEM (Finite Element Method) for physics simulation and IPC (Incremental Potential Contact) as the contact model. Below is the demonstration of our simulator for the peg-in-hole task. We have verified the zero-shot Sim2Real performance of the simulator.
              </p>
              <div style="text-align: center;">
              <img src="sim_real_align.gif" alt=""  width="569" height="320">
              </div>
              <h3>Phase 2</h3>
              <p>
              The top <i>N</i> participants’ algorithms will be evaluated on a physical robot. We use <a href="https://www.gelsight.com/gelsightmini/">GelSight Mini</a>, a commercially available vision-based tactile sensor, in our challenge. The input and output format of the real environment will be the same as the simulation. 
              <div style="text-align: center;">
                <img src="image5.png" alt=""  height="320">
                </div>
              </p>
              <p>
              Each team will have three chances to upload the codes and evaluate. Full evaluation logs, including raw observations, groundtruth states, will be given to the participants, such that they can fine-tune their algorithms.
              </p>
            </div>
            <div class="section" id="tasks">
              <h2>Tasks</h2>
              <p>
                The challenge includes two tasks: peg insertion and lock opening. Similar to the  <a href="https://sapien.ucsd.edu/challenges/maniskill/">ManiSkill challenge</a>, we focus on evaluating the policy’s generalizability. Each task includes multiple objects of different shapes (e.g., the peg can be a cuboid or a trapezoid, and keys can have two or three teeth), which are unknown during evaluation. Therefore, the policy needs to be able to generalize to different plugs and keys.
              </p>
              <div style="text-align: center;">
                <img src="image4.gif" alt="Plug insertion"  height="200">
                <figcaption>Plug insertion</figcaption>
                <img src="image3.gif" alt="Lock opening"  height="200">
                <figcaption>Lock opening</figcaption>
              </div>
            </div>
            <div class="section" id="awards">
            <h2>Awards</h2>
            <ul>
              <li>1st: 2 <a href="https://www.gelsight.com/gelsightmini/">GelSight Mini</a></li>
              <li>2nd: 1 <a href="https://www.gelsight.com/gelsightmini/">GelSight Mini</a></li>
              <li>3rd: 1 <a href="https://www.gelsight.com/products/#digittactilesensor">DIGIT Tactile Sensor</a></li>
            </ul>
            </div>
            <div class="section" id="metrics">
              <h2>Metrics</h2>
              <p>
                Participating entries will be evaluated on <i>K</i> initial states. Each state will repeat <i>T</i> times. An episode is successful if the peg is in the hole and all the pins in the lock are lifted up. For the real-world robot, a torque sensor is used. If at any point the torque on the gripper is larger a than threshold, the task fails immediately. Then, we will have an array of success rate. We use the average rank of the <i>K</i> initial states as the final metric for comparing submissions.
              </p>
              <p>
                For Phase1, the participants run the evaluation script on their own computer and submit the log files to the challenge committee.
              </p>
              <p>
                We reserve the right to use additional metrics to choose winners in case of statistically insignificant differences.
              </p>
            </div>
            <div class="section" id="schedule">
              <h2>Schedule</h2>
              <ul>
                <li>2024/01/31 Challenge release
                </li>
                <li>2024/03/31 Phase 1 Deadline
                </li>
                <li>2024/04/30 Phase 2 Deadline
                </li>
                <li>2024/05/13 Award Ceremony on the 5th ViTac workshop in <a href="https://2024.ieee-icra.org/">ICRA 2024</a>
                </li>
              </ul>
            </div>
            <div class="section" id="organizers">
                <h2>Organizers</h2>
                <h3>Faculty</h3>
                <div class="people">
                  <a href="https://callmeray.github.io/homepage/">
                    <img src="../assets/rui_chen.png">
                    <div>Rui Chen</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="https://cseweb.ucsd.edu/~haosu/">
                    <img src="../assets/haosu.jpeg">
                    <div>Hao Su</div>
                    <div class="aff">UC San Diego</div>
                  </a>                  
                  <a href="https://shanluo.github.io/">
                    <img src="../assets/shan_luo.png">
                    <div>Shan Luo</div>
                    <div class="aff">King's College London</div>
                  </a>
                  <a href="https://lepora.com/">
                    <img src="../assets/nathan_lepora.jpg">
                    <div>Nathan Lepora</div>
                    <div class="aff">University of Bristol</div>
                  </a>
                  <a href="https://lasr.org/">
                    <img src="../assets/roberto_calandra.jpg">
                    <div>Roberto Calandra</div>
                    <div class="aff">TU Dresden</div>
                  </a>
                  <a href="https://www.me.tsinghua.edu.cn/en/info/1081/1202.htm">
                    <img src="../assets/jing_xu.jpg">
                    <div>Jing Xu</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                </div>
                <h3>Student</h3>
                <div class="people">
                  <a href="">
                    <img src="../assets/weihang_chen.jpg">
                    <div>Weihang Chen</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="">
                    <img src="../assets/yuhao_liu.jpg">
                    <div>YuHao Liu</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                  <a href="https://rabbit-hu.github.io/">
                    <img src="../assets/xiaodi_yuan.jpg">
                    <div>Xiaodi Yuan</div>
                    <div class="aff">UC San Diego</div>
                  </a>
                  <a href="">
                    <img src="../assets/chuanyu_li.jpg">
                    <div>Chuanyu Li</div>
                    <div class="aff">Sichuan University</div>
                  </a>                  
                  <a href="https://www.fbxiang.com">
                    <img src="../assets/fanbo_xiang.jpg">
                    <div>Fanbo Xiang</div>
                    <div class="aff">UC San Diego</div>
                  </a>
                  <a href="">
                    <img src="../assets/mai_zhang.jpg">
                    <div>Mai Zhang</div>
                    <div class="aff">Tsinghua University</div>
                  </a>
                </div>
            </div>
            <h2>Citation</h2>
            <p>If you use our work in your research please cite the following</p>
            <pre>@ARTICLE{chen2024tactilesim2real,
              author={Chen, Weihang and Xu, Jing and Xiang, Fanbo and Yuan, Xiaodi and Su, Hao and Chen, Rui},
              journal={IEEE Transactions on Robotics}, 
              title={General-Purpose Sim2Real Protocol for Learning Contact-Rich Manipulation With Marker-Based Visuotactile Sensors}, 
              year={2024},
              volume={},
              number={},
              pages={1-18},
              doi={10.1109/TRO.2024.3352969}}
            </pre>
        </div>
        
        <div class="foot">
          © ManiSkill-ViTac: Vision-based-Tactile Manipulation Skill Learning Challenge 2024
        </div>
    </body>
</html>
